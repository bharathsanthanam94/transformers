{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # Your     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # journey  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # starts   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # with     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # one      (x^5)\n",
    "        [0.05, 0.80, 0.55],\n",
    "    ]  # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4829, -0.0462],\n",
      "        [-0.4823, -0.0540],\n",
      "        [-0.4823, -0.0539],\n",
      "        [-0.4827, -0.0537],\n",
      "        [-0.4830, -0.0520],\n",
      "        [-0.4825, -0.0543]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# using nn.Linear() as way to define the weight matrices\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        # initialize the weight matrices\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute query, key and value vectors\n",
    "        keys = self.W_key(x)  # multiply 6X3 with 3x2 matrices\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # compute attention scores\n",
    "        attention_scores = queries @ keys.T\n",
    "\n",
    "        # compute scaled dot product attention\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1] ** 0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        # compute context vectors\n",
    "        context_vectors = attention_weights @ values\n",
    "        return context_vectors\n",
    "\n",
    "\n",
    "# create an instance of the SelfAttention_v1 class\n",
    "d_in = 3\n",
    "d_out = 2\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out, qkv_bias=False)\n",
    "context_vectors = sa_v2(inputs)\n",
    "print(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1718, 0.1573, 0.1567, 0.1766, 0.1560, 0.1816],\n",
      "        [0.1542, 0.1584, 0.1578, 0.1835, 0.1572, 0.1888],\n",
      "        [0.1545, 0.1586, 0.1580, 0.1831, 0.1574, 0.1884],\n",
      "        [0.1574, 0.1626, 0.1622, 0.1765, 0.1619, 0.1793],\n",
      "        [0.1623, 0.1641, 0.1639, 0.1722, 0.1637, 0.1738],\n",
      "        [0.1548, 0.1605, 0.1600, 0.1805, 0.1596, 0.1846]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# causal attention  step 1\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0.1718, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1542, 0.1584, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1545, 0.1586, 0.1580, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.1626, 0.1622, 0.1765, 0.0000, 0.0000],\n",
      "        [0.1623, 0.1641, 0.1639, 0.1722, 0.1637, 0.0000],\n",
      "        [0.1548, 0.1605, 0.1600, 0.1805, 0.1596, 0.1846]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# causal attention step 2, masking attention weights by multiplyig with a lower triangular matrix\n",
    "block_size = attn_scores.shape[0]\n",
    "mask_simple = torch.ones(block_size, block_size)\n",
    "simple_mask = torch.tril(mask_simple)\n",
    "print(simple_mask)\n",
    "\n",
    "masked_attn_weights = attn_weights * simple_mask\n",
    "print(masked_attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4933, 0.5067, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3279, 0.3367, 0.3354, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2390, 0.2468, 0.2463, 0.2680, 0.0000, 0.0000],\n",
      "        [0.1964, 0.1986, 0.1984, 0.2084, 0.1982, 0.0000],\n",
      "        [0.1548, 0.1605, 0.1600, 0.1805, 0.1596, 0.1846]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# causal attention step 3, re normalization is done again to make the rows sum to 1\n",
    "row_sums = masked_attn_weights.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_attn_weights / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "apply inf to masks: tensor([[-0.1928,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.3860, -0.3479,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.3781, -0.3406, -0.3463,    -inf,    -inf,    -inf],\n",
      "        [-0.2345, -0.1888, -0.1920, -0.0723,    -inf,    -inf],\n",
      "        [-0.1290, -0.1131, -0.1149, -0.0451, -0.1165,    -inf],\n",
      "        [-0.3241, -0.2726, -0.2771, -0.1067, -0.2809, -0.0749]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn scores without mask: tensor([[-0.1928, -0.3181, -0.3234, -0.1538, -0.3291, -0.1143],\n",
      "        [-0.3860, -0.3479, -0.3537, -0.1406, -0.3587, -0.0996],\n",
      "        [-0.3781, -0.3406, -0.3463, -0.1376, -0.3511, -0.0975],\n",
      "        [-0.2345, -0.1888, -0.1920, -0.0723, -0.1945, -0.0505],\n",
      "        [-0.1290, -0.1131, -0.1149, -0.0451, -0.1165, -0.0319],\n",
      "        [-0.3241, -0.2726, -0.2771, -0.1067, -0.2809, -0.0749]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "attn scores after masking: tensor([[-0.1928,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.3860, -0.3479,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.3781, -0.3406, -0.3463,    -inf,    -inf,    -inf],\n",
      "        [-0.2345, -0.1888, -0.1920, -0.0723,    -inf,    -inf],\n",
      "        [-0.1290, -0.1131, -0.1149, -0.0451, -0.1165,    -inf],\n",
      "        [-0.3241, -0.2726, -0.2771, -0.1067, -0.2809, -0.0749]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn weights after normalization: tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4933, 0.5067, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3279, 0.3367, 0.3354, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2390, 0.2468, 0.2463, 0.2680, 0.0000, 0.0000],\n",
      "        [0.1964, 0.1986, 0.1984, 0.2084, 0.1982, 0.0000],\n",
      "        [0.1548, 0.1605, 0.1600, 0.1805, 0.1596, 0.1846]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# more efficient implementation of causal attention\n",
    "# make the values above  the diagonal to be negative infinity before applying softmax\n",
    "mask = torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
    "print(mask)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(\"apply inf to masks:\", masked)\n",
    "print(\"attn scores without mask:\", attn_scores)\n",
    "print(\"attn scores after masking:\", masked)\n",
    "\n",
    "# apply softmax function to the masked results\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
    "print(\"attn weights after normalization:\", attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before dropout: tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "after dropout: tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6707, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4936, 0.0000, 0.5360, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3973, 0.3968, 0.4168, 0.3963, 0.0000],\n",
      "        [0.3096, 0.3210, 0.0000, 0.0000, 0.3192, 0.3692]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# one more change in causal attention with dropout to prevent overfitting\n",
    "torch.manual_seed(123)\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(\"before dropout:\", example)\n",
    "print(\"after dropout:\", dropout(example))\n",
    "\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# create a simple batch by duplicating the inputs tensor\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries shape: torch.Size([2, 6, 2])\n",
      "keys shape: torch.Size([2, 6, 2])\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# complete python class implementing causal attention with dropout\n",
    "\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, block_size, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out)\n",
    "        self.W_key = nn.Linear(d_in, d_out)\n",
    "        self.W_value = nn.Linear(d_in, d_out)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_token, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        print(\"queries shape:\", queries.shape)\n",
    "        print(\"keys shape:\", keys.shape)\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(self.mask[:num_token, :num_token].bool(), -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vectors = attn_weights @ values\n",
    "        return context_vectors\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "block_size = batch.shape[1]\n",
    "d_in = 3\n",
    "d_out = 2\n",
    "ca = CausalAttention(d_in, d_out, block_size, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "# context_vecs.shape: torch.Size([2, 6, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mesh_transformer2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
